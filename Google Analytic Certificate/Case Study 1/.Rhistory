diffima = abs(ima_1.5-predima)
#3. Compare the five forecasts with the actual values that you set aside
diffima = abs(ima_1.5-predima)
predima = output_4$pred
diffima = abs(ima_1.5-predima)
abs
diffima = abs(ima_1.5-predima)
output_4 = plot(mod_4, n.ahead = 5, pch=18)
output_4$pred
predima = output_4$pred
#3. Compare the five forecasts with the actual values that you set aside
diffima <- abs(ima_1.5 - predima)
(a) If sales for 2005,2006, and 2007 were $9 million, $11 million, and $10 million,
#(a) If sales for 2005,2006, and 2007 were $9 million, $11 million, and $10 million, respectively, forecast sales for 2008, 2009 and 2010.
#Calculate
acmecal = c(9,11,10)
thetazero = 5
phi1 = 1.1
phi2 = -0.5
for (i in 1:3) {
n = length(acmecal)
forecast_acme = thetazero + phi1 * acmecal[n] + phi2 * acmecal[n - 1]
# Update
acmecal <- c(acmecal, forecast_acme)
print(paste('The forcast sales for',2007+i,'is',forecast_acme))
}
#Plot
acme = c(9,11,10)
mod3 = arima(acme, order = c(2,0,0), fixed = c(phi1,phi2,thetazero/(1-phi1-
phi2)))
mod3
output3 = plot(mod3,n.ahead = 3,pch=18)
print(paste('the difference of forecasts and the actual values at l =',i,
'is', diffima[i]))
mod3
#Calculate
acmecal = c(9,11,10)
thetazero = 5
phi1 = 1.1
phi2 = -0.5
for (i in 1:3) {
n = length(acmecal)
forecast_acme = thetazero + phi1 * acmecal[n] + phi2 * acmecal[n - 1]
# Update
acmecal <- c(acmecal, forecast_acme)
print(paste('The forcast sales for',2007+i,'is',forecast_acme))
}
#Plot
acme = c(9,11,10)
mod3 = arima(acme, order = c(2,0,0), fixed = c(phi1,phi2,thetazero/(1-phi1-
phi2)))
mod3
output3 = plot(mod3,n.ahead = 3,pch=18)
set.seed(1234)
ima = arima.sim(n = 35, list(order = c(0,1,1), ma=-0.8))
# Using the first 30 values of the series to estimate ARIMA parameters
ima_1.3 = window(ima, end = 30)
mod_4 = arima(ima_1.3, order = c(0,1,1), method = "ML")
# Extracting estimated coefficients
theta_4 = mod_4$coef
print(paste("The maximum likelihood estimates of theta is", theta_4))
# Forecasting the next five values using the estimated model
forecast_values <- predict(mod_4, n.ahead = 5)$pred
# Comparing the forecasts with the actual values
diffima <- abs(ima[31:35] - forecast_values)
for (i in 1:5) {
print(paste('the difference of forecasts and the actual values at l =', i, 'is', diffima[i]))
}
# Plotting the forecasts together with the original series
plot(mod_4, n.ahead = 5, pch = 18)
lines(0:35, ima, col = 'red', type = 'b')
# Repeat with a new simulated series
set.seed(12123)
ima_new = arima.sim(n = 35, list(order = c(0,1,1), ma=-0.8))
# Using the first 30 values of the new series to estimate ARIMA parameters
ima_new_1.3 = window(ima_new, end = 30)
mod_4_new = arima(ima_new_1.3, order = c(0,1,1), method = "ML")
# Extracting estimated coefficients
theta_4_new = mod_4_new$coef
print(paste("The maximum likelihood estimates of theta for the new series is", theta_4_new))
# Forecasting the next five values using the estimated model for the new series
forecast_values_new <- predict(mod_4_new, n.ahead = 5)$pred
# Comparing the forecasts with the actual values for the new series
diffima_new <- abs(ima_new[31:35] - forecast_values_new)
for (i in 1:5) {
print(paste('the difference of forecasts and the actual values at l =', i, 'is', diffima_new[i]))
}
# Plotting the forecasts together with the original series for the new series
plot(mod_4_new, n.ahead = 5, pch = 18)
lines(0:35, ima_new, col = 'red', type = 'b')
set.seed(1234)
ima = arima.sim(n = 35, list(order = c(0,1,1), ma=-0.8))
# Using the first 30 values of the series to estimate ARIMA parameters
ima_1.3 = window(ima, end = 30)
mod_4 = arima(ima_1.3, order = c(0,1,1), method = "ML")
# Extracting estimated coefficients
theta_4 = mod_4$coef
print(paste("The maximum likelihood estimates of theta is", theta_4))
# Forecasting the next five values using the estimated model
forecast_values <- predict(mod_4, n.ahead = 5)$pred
# Comparing the forecasts with the actual values
diffima <- abs(ima[31:35] - forecast_values)
for (i in 1:5) {
print(paste('the difference of forecasts and the actual values at l =', i, 'is', diffima[i]))
}
# Plotting the forecasts together with the original series
plot(mod_4, n.ahead = 5, pch = 18)
lines(0:35, ima, col = 'red', type = 'b')
# Repeat with a new simulated series
set.seed(12123)
ima_new = arima.sim(n = 35, list(order = c(0,1,1), ma=-0.8))
# Using the first 30 values of the new series to estimate ARIMA parameters
ima_new_1.3 = window(ima_new, end = 30)
mod_4_new = arima(ima_new_1.3, order = c(0,1,1), method = "ML")
# Extracting estimated coefficients
theta_4_new = mod_4_new$coef
print(paste("The maximum likelihood estimates of theta for the new series is", theta_4_new))
# Forecasting the next five values using the estimated model for the new series
forecast_values_new <- predict(mod_4_new, n.ahead = 5)$pred
# Comparing the forecasts with the actual values for the new series
diffima_new <- abs(ima_new[31:35] - forecast_values_new)
for (i in 1:5) {
print(paste('the difference of forecasts and the actual values at l =', i, 'is', diffima_new[i]))
}
# Plotting the forecasts together with the original series for the new series
plot(mod_4_new, n.ahead = 5, pch = 18)
lines(0:35, ima_new, col = 'red', type = 'b')
data <- "https://raw.githubusercontent.com/prabeshdhakal/Introductory-Time-Series-with-R-Datasets/master/Maine.dat"
data
CBE <- read.table(data, header = T)
CBE
Seasonal.Dataset
data(Seasonal.Dataset)
library(Seasonal.Dataset)
# Get confidence intervals for the estimated marginal means
emm_summary <- summary(we_emm)
data <- read.csv("data.csv")
data <- read.csv("data.csv")
# Load necessary libraries
library(quantmod)
library(dplyr)
library(zoo)
# Define SET100 stock symbols
symbols <- c("ADVANC.BK", "AEONTS.BK", "AMATA.BK", "AOT.BK", "AP.BK", "BANPU.BK",
"BBL.BK", "BCH.BK", "BCP.BK", "BCPG.BK", "BDMS.BK", "BEC.BK",
"BEM.BK", "BGRIM.BK", "BH.BK", "BJC.BK", "BTS.BK", "CBG.BK",
"CENTEL.BK", "CHG.BK", "CK.BK", "CKP.BK", "COM7.BK", "CPALL.BK",
"CPF.BK", "CPN.BK", "CRC.BK", "DELTA.BK", "EA.BK",
"EGCO.BK", "GLOBAL.BK", "GPSC.BK", "GULF.BK", "HMPRO.BK",
"INTUCH.BK", "IRPC.BK", "IVL.BK", "JMART.BK", "JMT.BK", "KBANK.BK",
"KCE.BK", "KKP.BK", "KTB.BK", "KTC.BK", "LH.BK", "MINT.BK",
"MTC.BK", "ORI.BK", "OSP.BK", "PLANB.BK", "PRM.BK", "PTG.BK",
"PTT.BK", "PTTEP.BK", "PTTGC.BK", "QH.BK", "RATCH.BK", "RS.BK",
"SAWAD.BK", "SCB.BK", "SCC.BK", "SCGP.BK", "SGP.BK", "SPALI.BK",
"SPRIME.BK", "STA.BK", "STEC.BK", "STGT.BK", "STPI.BK", "SUPER.BK",
"TASCO.BK", "TCAP.BK", "THANI.BK", "TISCO.BK",  "TOA.BK",
"TOP.BK", "TQM.BK", "TRUE.BK", "TTW.BK", "TU.BK", "VGI.BK", "WHA.BK")
# Download data for SET Index
getSymbols("^SET.BK", from = "2020-01-01", to = Sys.Date(), src = "yahoo")
set_index <- Cl(SET.BK)
data <- na.omit(set_index)
# Create data.frame to store returns
returns <- data.frame(Date = index(data))
# Calculate daily returns for the market index
set_returns <- dailyReturn(data)
set_returns <- na.omit(set_returns)  # Fill missing values with last observed value
returns$SET <- set_returns
returns <- na.omit(returns)
# Download and calculate returns for each stock
for (symbol in symbols) {
getSymbols(symbol, from = "2020-01-01", to = Sys.Date(), src = "yahoo")
stock <- get(symbol)
stock <- na.omit(stock)  # Remove missing values
stock_return <- dailyReturn(Cl(stock))
stock_return <- na.omit(stock_return)  # Fill missing values with last observed value
returns <- merge(returns, data.frame(Date = index(stock_return), stock_return), by = "Date")
}
# Set column names
colnames(returns) <- c("Date", "SET", symbols)
# Remove remaining rows with NA
returns <- na.omit(returns)
# ฟังก์ชันเพื่อคำนวณ NCC #อย่าลืมกำหนดค่่า rm
ncc_test <- function(Rm, Ri) {
mu_i <- mean(Ri)
mu_m <- mean(Rm)
delta_i <- sum((Ri - mu_i)^2) / sum((Rm - mu_m) * (Ri - mu_i))
if (delta_i >= 1 && delta_i <= 10) {
ncc_holds <- TRUE
for (gamma in 2:10) {
Rm_gamma <- Rm^(-gamma)
rho <- cor(Rm_gamma, Ri)
t_test <- cor.test(Rm_gamma, Ri, alternative = "greater")
if (t_test$p.value < 0.05) {
ncc_holds <- FALSE
break
}
}
if (ncc_holds) {
return("The NCC holds")
} else {
return("The NCC does not hold")
}
} else {
return("The NCC does not hold")
}
}
# ตัวอย่างการใช้งานฟังก์ชัน NCC สำหรับหุ้นใน SET100
for (symbol in symbols) {
result <- ncc_test(returns$SET, returns[[symbol]])
print(paste(symbol, ":", result))
}
returns[[symbol]]
returns
returns[[symbol]]
setwd("C:/Users/USER/Desktop/PROJECT NONGEARNZ")
\documentclass{beamer}
library(tidyverse)
library(palmerpenguins)
library(tidyverse)
library(palmerpenguins)
library(tidyverse)
library(palmerpenguins)
library(tidyverse)
library(palmerpenguins)
library(tidyverse)
library(palmerpenguins)
library(tidyverse)
library(palmerpenguins)
library(tidyverse)
library(palmerpenguins)
ggplot(data = penguins) +
geom_point(mapping = aes(x = flipper_length_mm, y = body_mass_g))
library(ggplot2)
library(palmerpenguins)
data(penguins)
View(penguins)
data(penguins)
View(penguins)
library(tidyverse)
library(palmerpenguins)
glimpse(penguins)
penguins %>%
drop_na(sex) %>%
ggplot(aes(x=flipper_length_mm,y=body_mass_g)) +
geom_point(aes(color=species),
shape=species) +
facet_wrap(~sex)
penguins
library(tidyverse)
library(palmerpenguins)
glimpse(penguins)
ggplot(data=penguins,aes(x=flipper_length_mm,y=body_mass_g))+
geom_point(color="purple")
ggplot(data=penguins,aes(x=flipper_length_mm,y=body_mass_g))+
geom_point(aes(shape=species))
ggplot(data=penguins,aes(x=flipper_length_mm,y=body_mass_g))+
geom_point(aes(color=species,
shape=species)) +
facet_wrap(~sex)
```{r}
penguins %>%
drop_na(sex) %>%
ggplot(data=penguins,aes(x=flipper_length_mm,y=body_mass_g))+
geom_point(aes(color=species,
shape=species)) +
facet_wrap(~sex)
penguins
penguins %>%
drop_na(sex) %>%
ggplot(data=penguins,aes(x=flipper_length_mm,y=body_mass_g))+
geom_point(aes(color=species,
shape=species)) +
facet_wrap(~sex)
penguins %>%
drop_na(sex) %>%
ggplot(aes(x=flipper_length_mm,y=body_mass_g))+
geom_point(aes(color=species,
shape=species)) +
facet_wrap(~sex)
setwd("C:/Users/USER/Desktop/Google Analytic Certificate/Case Study 1")
library(tidyverse)  #helps wrangle data
# Use the conflicted package to manage conflicts
library(conflicted)
# Set dplyr::filter and dplyr::lag as the default choices
conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
#=====================
# STEP 1: COLLECT DATA
#=====================
# # Upload Divvy datasets (csv files) here
q1_2019 <- read_csv("Divvy_Trips_2019_Q1.csv")
q1_2020 <- read_csv("Divvy_Trips_2020_Q1.csv")
colnames(q1_2019)
colnames(q1_2020)
(q1_2019 <- rename(q1_2019
,ride_id = trip_id
,rideable_type = bikeid
,started_at = start_time
,ended_at = end_time
,start_station_name = from_station_name
,start_station_id = from_station_id
,end_station_name = to_station_name
,end_station_id = to_station_id
,member_casual = usertype
))
# Inspect the dataframes and look for incongruencies
str(q1_2019)
str(q1_2020)
q1_2019 <-  mutate(q1_2019, ride_id = as.character(ride_id)
,rideable_type = as.character(rideable_type))
all_trips <- bind_rows(q1_2019, q1_2020)
View(all_trips)
# Remove lat, long, birthyear, and gender fields as this data was dropped beginning in 2020
all_trips <- all_trips %>%
select(-c(start_lat, start_lng, end_lat, end_lng, birthyear, gender,  "tripduration"))
colnames(all_trips)
nrow(all_trips)  #How many rows are in data frame?
dim(all_trips)  #Dimensions of the data frame?
head(all_trips)  #See the first 6 rows of data frame.  Also tail(all_trips)
str(all_trips)  #See list of columns and data types (numeric, character, etc)
summary(all_trips)  #Statistical summary of data. Mainly for numerics
# In the "member_casual" column, replace "Subscriber" with "member" and "Customer" with "casual"
# Before 2020, Divvy used different labels for these two types of riders ... we will want to make our dataframe consistent with their current nomenclature
# N.B.: "Level" is a special property of a column that is retained even if a subset does not contain any values from a specific level
# Begin by seeing how many observations fall under each usertype
table(all_trips$member_casual)
table(all_trips$member_casual)
all_trips <-  all_trips %>%
mutate(member_casual = recode(member_casual
,"Subscriber" = "member"
,"Customer" = "casual"))
# Check to make sure the proper number of observations were reassigned
table(all_trips$member_casual)
table(all_trips$member_casual)
all_trips$date <- as.Date(all_trips$started_at)
all_trips
View(all_trips)
all_trips$month <- format(as.Date(all_trips$date), "%m")
all_trips$day <- format(as.Date(all_trips$date), "%d")
all_trips$year <- format(as.Date(all_trips$date), "%Y")
all_trips$day_of_week <- format(as.Date(all_trips$date), "%A")
View(all_trips)
all_trips$ride_length <- difftime(all_trips$ended_at,all_trips$started_at)
str(all_trips)
# Convert "ride_length" from Factor to numeric so we can run calculations on the data
is.factor(all_trips$ride_length)
all_trips$ride_length <- as.numeric(as.character(all_trips$ride_length))
is.numeric(all_trips$ride_length)
# Remove "bad" data
# The dataframe includes a few hundred entries when bikes were taken out of docks and checked for quality by Divvy or ride_length was negative
# We will create a new version of the dataframe (v2) since data is being removed
# https://www.datasciencemadesimple.com/delete-or-drop-rows-in-r-with-conditions-2/
all_trips_v2 <- all_trips[!(all_trips$start_station_name == "HQ QR" | all_trips$ride_length<0),]
#=====================================
# STEP 4: CONDUCT DESCRIPTIVE ANALYSIS
#=====================================
# Descriptive analysis on ride_length (all figures in seconds)
mean(all_trips_v2$ride_length) #straight average (total ride length / rides)
median(all_trips_v2$ride_length) #midpoint number in the ascending array of ride lengths
max(all_trips_v2$ride_length) #longest ride
min(all_trips_v2$ride_length) #shortest ride
# You can condense the four lines above to one line using summary() on the specific attribute
summary(all_trips_v2$ride_length)
# Compare members and casual users
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual, FUN = mean)
# Compare members and casual users
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual, FUN = mean)
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual, FUN = median)
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual, FUN = max)
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual, FUN = min)
# See the average ride time by each day for members vs casual users
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual + all_trips_v2$day_of_week, FUN = mean)
# Notice that the days of the week are out of order. Let's fix that.
all_trips_v2$day_of_week <- ordered(all_trips_v2$day_of_week, levels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
# Now, let's run the average ride time by each day for members vs casual users
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual + all_trips_v2$day_of_week, FUN = mean)
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual + all_trips_v2$day_of_week, FUN = mean)
all_trips_v2 <-  all_trips_v2 %>%
mutate(day_of_week = recode(day_of_week
,"อาทิตย์" = "Sunday"
,"จันทร์" = "Monday",
"อังคาร" = "Tuesday",
"พุธ" = "Wednesday",
"พฤหัสบดี" = "Thursday",
"ศุกร์" = "Friday",
"เสาร์" = "Saturday"))
day_of_week
all_trips_v2$day_of_week
all_trips_v2$day_of_week <- ordered(all_trips_v2$day_of_week, levels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual + all_trips_v2$day_of_week, FUN = mean)
all_trips_v2
View(all_trips)
View(all_trips_v2)
all_trips_v2 <-  all_trips_v2 %>%
mutate(day_of_week = recode(day_of_week
,"อาทิตย์" = "Sunday"
,"จันทร์" = "Monday",
"อังคาร" = "Tuesday",
"พุธ" = "Wednesday",
"พฤหัสบดี" = "Thursday",
"ศุกร์" = "Friday",
"เสาร์" = "Saturday"))
# Add a "ride_length" calculation to all_trips (in seconds)
# https://stat.ethz.ch/R-manual/R-devel/library/base/html/difftime.html
all_trips$ride_length <- difftime(all_trips$ended_at,all_trips$started_at)
# Inspect the structure of the columns
str(all_trips)
all_trips_v2 <-  all_trips_v2 %>%
mutate(day_of_week = recode(day_of_week
,"อาทิตย์" = "Sunday"
,"จันทร์" = "Monday",
"อังคาร" = "Tuesday",
"พุธ" = "Wednesday",
"พฤหัสบดี" = "Thursday",
"ศุกร์" = "Friday",
"เสาร์" = "Saturday"))
all_trips_v2
all_trips <-  all_trips %>%
mutate(day_of_week = recode(day_of_week
,"อาทิตย์" = "Sunday"
,"จันทร์" = "Monday",
"อังคาร" = "Tuesday",
"พุธ" = "Wednesday",
"พฤหัสบดี" = "Thursday",
"ศุกร์" = "Friday",
"เสาร์" = "Saturday"))
# Add a "ride_length" calculation to all_trips (in seconds)
# https://stat.ethz.ch/R-manual/R-devel/library/base/html/difftime.html
all_trips$ride_length <- difftime(all_trips$ended_at,all_trips$started_at)
# Inspect the structure of the columns
str(all_trips)
# Inspect the structure of the columns
str(all_trips)
# Add a "ride_length" calculation to all_trips (in seconds)
# https://stat.ethz.ch/R-manual/R-devel/library/base/html/difftime.html
all_trips$ride_length <- difftime(all_trips$ended_at,all_trips$started_at)
is.factor(all_trips$ride_length)
all_trips$ride_length <- as.numeric(as.character(all_trips$ride_length))
is.numeric(all_trips$ride_length)
# Remove "bad" data
# The dataframe includes a few hundred entries when bikes were taken out of docks and checked for quality by Divvy or ride_length was negative
# We will create a new version of the dataframe (v2) since data is being removed
# https://www.datasciencemadesimple.com/delete-or-drop-rows-in-r-with-conditions-2/
all_trips_v2 <- all_trips[!(all_trips$start_station_name == "HQ QR" | all_trips$ride_length<0),]
mean(all_trips_v2$ride_length) #straight average (total ride length / rides)
median(all_trips_v2$ride_length) #midpoint number in the ascending array of ride lengths
max(all_trips_v2$ride_length) #longest ride
min(all_trips_v2$ride_length) #shortest ride
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual, FUN = mean)
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual, FUN = median)
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual, FUN = max)
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual, FUN = min)
# See the average ride time by each day for members vs casual users
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual + all_trips_v2$day_of_week, FUN = mean)
# Notice that the days of the week are out of order. Let's fix that.
all_trips_v2$day_of_week <- ordered(all_trips_v2$day_of_week, levels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
# Now, let's run the average ride time by each day for members vs casual users
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual + all_trips_v2$day_of_week, FUN = mean)
all_trips_v2 %>%
mutate(weekday = wday(started_at, label = TRUE))
all_trips_v2
View(all_trips_v2)
all_trips_v2 %>%
mutate(weekday = wday(started_at, label = TRUE)) %>%  #creates weekday field using wday()
group_by(member_casual, weekday)
all_trips_v2
print(n = 100)
# analyze ridership data by type and weekday
all_trips_v2 %>%
mutate(weekday = wday(started_at, label = TRUE)) %>%  #creates weekday field using wday()
group_by(member_casual, weekday) %>%  #groups by usertype and weekday
summarise(number_of_rides = n()							#calculates the number of rides and average duration
,average_duration = mean(ride_length)) %>% 		# calculates the average duration
arrange(member_casual, weekday)
all_trips_v2 %>%
mutate(weekday = wday(started_at, label = TRUE)) %>%
group_by(member_casual, weekday) %>%
summarise(number_of_rides = n()
,average_duration = mean(ride_length)) %>%
arrange(member_casual, weekday)  %>%
ggplot(aes(x = weekday, y = number_of_rides, fill = member_casual)) +
geom_col(position = "dodge")
all_trips_v2 %>%
mutate(weekday = wday(started_at, label = TRUE)) %>%
group_by(member_casual, weekday) %>%
summarise(number_of_rides = n()
,average_duration = mean(ride_length)) %>%
arrange(member_casual, weekday)  %>%
ggplot(aes(x = weekday, y = average_duration, fill = member_casual)) +
geom_col(position = "dodge")
#=================================================
# STEP 5: EXPORT SUMMARY FILE FOR FURTHER ANALYSIS
#=================================================
# Create a csv file that we will visualize in Excel, Tableau, or my presentation software
# N.B.: This file location is for a Mac. If you are working on a PC, change the file location accordingly (most likely "C:\Users\YOUR_USERNAME\Desktop\...") to export the data. You can read more here: https://datatofish.com/export-dataframe-to-csv-in-r/
counts <- aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual + all_trips_v2$day_of_week, FUN = mean)
counts
write.csv(counts, file = 'avg_ride_length.csv')
# See the average ride time by each day for members vs casual users
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual + all_trips_v2$day_of_week, FUN = mean)
# Now, let's run the average ride time by each day for members vs casual users
aggregate(all_trips_v2$ride_length ~ all_trips_v2$member_casual + all_trips_v2$day_of_week, FUN = mean)
# analyze ridership data by type and weekday
all_trips_v2 %>%
mutate(weekday = wday(started_at, label = TRUE)) %>%  #creates weekday field using wday()
group_by(member_casual, weekday) %>%  #groups by usertype and weekday
summarise(number_of_rides = n()							#calculates the number of rides and average duration
,average_duration = mean(ride_length)) %>% 		# calculates the average duration
arrange(member_casual, weekday)
nrow(all_trips_v2$ride_id)
n(all_trips_v2$ride_id)
nrow(all_trips_v2)
